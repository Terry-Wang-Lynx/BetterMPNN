import torch
import torch.optim as optim
import numpy as np
import copy
import random
import matplotlib.pyplot as plt

# Import necessary modules from the EvoDiff project
from evodiff.pretrained import OA_DM_38M  # Using a smaller model for quick testing
from evodiff.utils import Tokenizer

'''
================================================================================
SECTION 1: Core Functions
================================================================================
'''

def get_log_probs(model, sequence, decode_order, tokenizer, device='cuda'):
    """
    Calculate the log probability of each token in the given sequence when generated by OADM at specific positions and order.
    """
    all_aas = tokenizer.all_aas
    target_tokens = torch.tensor(tokenizer.tokenizeMSA(sequence), dtype=torch.long, device=device)
    sequence_list = list(sequence)
    for pos in decode_order:
        sequence_list[pos] = '#'
    masked_sequence = ''.join(sequence_list)
    input_tokens = torch.tensor(tokenizer.tokenizeMSA(masked_sequence), dtype=torch.long, device=device)
    log_p_list = []
    for i in decode_order:
        timestep = torch.tensor([0], device=device)
        input_for_model = input_tokens.clone()
        prediction = model(input_for_model.unsqueeze(0), timestep)
        logits = prediction[:, i, :len(all_aas) - 6]
        log_p_distribution = torch.nn.functional.log_softmax(logits, dim=1)
        true_token_id = target_tokens[i]
        token_log_p = log_p_distribution[:, true_token_id]
        log_p_list.append(token_log_p)
        input_tokens[i] = true_token_id
    return torch.cat(log_p_list)

def generate_variant(model, sequence: str, mask_positions: list[int], decode_order: list[int],
                     tokenizer, device: str = 'cuda'):
    """
    Generate variants for a given protein sequence using the EvoDiff OADM model.
    """
    all_aas = tokenizer.all_aas
    sequence_list = list(sequence)
    for pos in mask_positions:
        sequence_list[pos] = '#'
    masked_sequence = ''.join(sequence_list)
    sample = torch.tensor(tokenizer.tokenizeMSA(masked_sequence), dtype=torch.long, device=device)
    for i in decode_order:
        timestep = torch.tensor([0], device=device)
        with torch.no_grad():  # No gradient calculation needed during generation
            prediction = model(sample.unsqueeze(0), timestep)
        logits = prediction[:, i, :len(all_aas) - 6]
        p = torch.nn.functional.softmax(logits, dim=1)
        new_token_id = torch.multinomial(p, num_samples=1)
        sample[i] = new_token_id.squeeze()
    return tokenizer.untokenize(sample)

def calculate_basic_amino_acid_percentage(sequence: str) -> float:
    """
    Calculate the percentage of basic amino acids in an amino acid sequence.
    """
    basic_amino_acids = {'R', 'K', 'H'}
    if not sequence:
        return 0.0
    sequence_length = len(sequence)
    basic_aa_count = 0
    for amino_acid in sequence.upper():
        if amino_acid in basic_amino_acids:
            basic_aa_count += 1
    percentage = (basic_aa_count / sequence_length) * 100
    return percentage


'''
================================================================================
SECTION 2: GRPO Reinforcement Learning Main Process (Multi-path Training)
================================================================================
'''

def grpo_finetune_evodiff():
    """
    Main function for fine-tuning the EvoDiff model using the GRPO algorithm.
    """
    # 1. Hyperparameter Settings
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    learning_rate = 1e-5
    training_steps = 300
    num_generations = 4  # Generate 4 variants for the same sequence each time
    num_paths_per_variant = 4  # New: Sample 4 paths per variant for training
    beta = 0.1           # Weight for KL divergence penalty
    # clip_value = 0.5

    print(f"Using device: {device}")

    # 2. Model and Tokenizer Loading
    print("Loading pretrained model...")
    model, _, tokenizer, _ = OA_DM_38M()
    model.to(device)
    model.train()

    ref_model = copy.deepcopy(model)
    ref_model.to(device)
    ref_model.eval()

    # 3. Optimizer Setup
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)

    # 4. Training Data Setup
    base_sequence = "PIAQIHILEGRSDEQKETLIREVSEAISRSLDAPLTSVRVIITEMAKGHFGIGGELASK"
    num_mutations = 15
    seq_len = len(base_sequence)

    print(f"Original sequence basic amino acid content: {calculate_basic_amino_acid_percentage(base_sequence):.2f}%")
    print("-" * 50)
    
    losses_history, rewards_history, kls_history = [], [], []

    # 5. GRPO Training Loop
    for step in range(training_steps):
        positions_to_mutate = sorted(random.sample(range(seq_len), num_mutations))
        
        # a. Generation (Rollout) - Using a single random path
        generation_order = positions_to_mutate.copy()
        random.shuffle(generation_order)
        
        variants = []
        for _ in range(num_generations):
            variant = generate_variant(
                model=model,
                sequence=base_sequence,
                mask_positions=positions_to_mutate,
                decode_order=generation_order,
                tokenizer=tokenizer,
                device=device
            )
            variants.append(variant)

        # b. Reward Calculation
        rewards = torch.tensor([calculate_basic_amino_acid_percentage(v) for v in variants], device=device)

        # c. Advantage Calculation (Within-group normalization)
        mean_reward = rewards.mean()
        std_reward = rewards.std()
        advantages = (rewards - mean_reward) / (std_reward + 1e-8)

        # d. Loss Calculation
        batch_loss = 0
        batch_kl_div = 0
        
        # Iterate through each generated variant and its corresponding advantage
        for i in range(num_generations):
            variant_sequence = variants[i]
            advantage = advantages[i]

            # For this variant, sample multiple paths for training
            for _ in range(num_paths_per_variant):
                # Randomly sample a new training path
                training_order = positions_to_mutate.copy()
                random.shuffle(training_order)
                
                # Calculate logp using the current model
                with torch.set_grad_enabled(True):
                    log_probs = get_log_probs(model, variant_sequence, training_order, tokenizer, device)
                
                # Calculate ref_logp using the reference model
                with torch.no_grad():
                    ref_log_probs = get_log_probs(ref_model, variant_sequence, training_order, tokenizer, device)

                policy_loss = -(log_probs * advantage).sum()
                
                kl_div_per_token = torch.exp(ref_log_probs - log_probs) - (ref_log_probs - log_probs) - 1
                kl_div = kl_div_per_token.sum()
                
                loss = policy_loss + beta * kl_div
                batch_loss += loss
                batch_kl_div += kl_div.item()

        # Average the loss over all variants and all paths
        total_loss = batch_loss / (num_generations * num_paths_per_variant)
        avg_kl_div = batch_kl_div / (num_generations * num_paths_per_variant)

        # e. Optimization
        optimizer.zero_grad()
        total_loss.backward()
        # torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)
        optimizer.step()
        
        losses_history.append(total_loss.item())
        rewards_history.append(mean_reward.item())
        kls_history.append(avg_kl_div)

        # f. Print Logs
        if step % 5 == 0:
            print(f"Step: {step+1}/{training_steps} | "
                  f"Average Reward: {mean_reward.item():.2f}% | "
                  f"Loss: {total_loss.item():.4f} | "
                  f"KL Divergence: {avg_kl_div:.4f}")

    # 6. Generate Charts After Training
    print("Training completed, generating charts...")
    fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(10, 15))
    fig.suptitle('GRPO', fontsize = 16)

    ax1.plot(losses_history, 'b-', label='total loss')
    ax1.set_xlabel('training steps'); ax1.set_ylabel('loss '); ax1.set_title('Training Loss'); ax1.legend()
    ax1.grid(True, linestyle='--', alpha = 0.7)
    
    ax2.plot(rewards_history, 'g-', label='average reward')
    ax2.set_xlabel('training steps'); ax2.set_ylabel('basic amino acid (%)'); ax2.set_title('Average Reward'); ax2.legend()
    ax2.grid(True, linestyle='--', alpha = 0.7)

    ax3.plot(kls_history, 'r-', label='KL')
    ax3.set_xlabel('training steps'); ax3.set_ylabel('KL'); ax3.set_title('KL Divergence'); ax3.legend()
    ax3.grid(True, linestyle='--', alpha = 0.7)

    # Moving Averages
    window = 3
    if len(losses_history) >= window:
        loss_ma = np.convolve(losses_history, np.ones(window)/window, mode='valid')
        ax1.plot(range(window-1, len(losses_history)), loss_ma, 'b--', label='moving averages')
        ax1.legend()
        
        reward_ma = np.convolve(rewards_history, np.ones(window)/window, mode='valid')
        ax2.plot(range(window-1, len(rewards_history)), reward_ma, 'g--', label='moving averages')
        ax2.legend()
        
        epochs_ma = np.convolve(kls_history, np.ones(window)/window, mode='valid')
        ax3.plot(range(window-1, len(kls_history)), epochs_ma, 'r--', label='moving averages')
        ax3.legend()

    plt.savefig("grpo_multi_path_training_metrics.png", dpi = 300, bbox_inches = 'tight')
    plt.show()


if __name__ == '__main__':
    grpo_finetune_evodiff()
