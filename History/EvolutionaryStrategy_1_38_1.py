import torch
import torch.optim as optim
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import copy
import random
import matplotlib.pyplot as plt
import os
import json
from collections import defaultdict
import subprocess
import glob
import math
import openpyxl
import time
import zipfile
import openpyxl.utils.exceptions

# Import necessary modules from EvoDiff project
from evodiff.pretrained import OA_DM_38M  # Can choose 640M for comparison
from evodiff.utils import Tokenizer

GROUP_NAME = "ES_1"  # Define group name

# Global variable for tracking training metrics history
TRAINING_METRICS_HISTORY = []

'''
================================================================================
SECTION 1: Core Functions
================================================================================
'''

def get_log_probs(model, sequence, decode_order, tokenizer, device='cuda'):
    """
    Calculate log probabilities of each token in the given sequence when generated by OADM at specific positions and order.
    """
    all_aas = tokenizer.all_aas
    target_tokens = torch.tensor(tokenizer.tokenizeMSA(sequence), dtype=torch.long, device=device)
    sequence_list = list(sequence)
    for pos in decode_order:
        sequence_list[pos] = '#'
    masked_sequence = ''.join(sequence_list)
    input_tokens = torch.tensor(tokenizer.tokenizeMSA(masked_sequence), dtype=torch.long, device=device)
    log_p_list = []
    for i in decode_order:
        timestep = torch.tensor([0], device=device)
        input_for_model = input_tokens.clone()
        prediction = model(input_for_model.unsqueeze(0), timestep)
        logits = prediction[:, i, :len(all_aas) - 6]
        log_p_distribution = torch.nn.functional.log_softmax(logits, dim=1)
        true_token_id = target_tokens[i]
        token_log_p = log_p_distribution[:, true_token_id]
        log_p_list.append(token_log_p)
        input_tokens[i] = true_token_id
    return torch.cat(log_p_list)

def generate_variant(model, sequence: str, mask_positions: list[int], decode_order: list[int],
                     tokenizer, device: str = 'cuda'):
    """
    Generate protein sequence variants using EvoDiff OADM model.
    """
    all_aas = tokenizer.all_aas
    sequence_list = list(sequence)
    for pos in mask_positions:
        sequence_list[pos] = '#'
    masked_sequence = ''.join(sequence_list)
    sample = torch.tensor(tokenizer.tokenizeMSA(masked_sequence), dtype=torch.long, device=device)
    for i in decode_order:
        timestep = torch.tensor([0], device=device)
        with torch.no_grad():  # No gradient calculation needed during generation
            prediction = model(sample.unsqueeze(0), timestep)
        logits = prediction[:, i, :len(all_aas) - 6]
        p = torch.nn.functional.softmax(logits, dim=1)
        new_token_id = torch.multinomial(p, num_samples=1)
        sample[i] = new_token_id.squeeze()
    return tokenizer.untokenize(sample)

def parse_all_scores_from_excel(filepath):
    """
    Extract all three metrics from specific cells in the summary.xlsx file.
    """
    try:
        # Use openpyxl to directly load workbook
        workbook = openpyxl.load_workbook(filepath, data_only=True)
        sheet = workbook.active 

        iptm_str = sheet['F3'].value
        ranking_score_str = sheet['H3'].value
        affinity_str = sheet['U2'].value

        iptm = float(iptm_str) if iptm_str is not None else 0.0
        ranking_score = float(ranking_score_str) if ranking_score_str is not None else 0.0
        affinity = float(affinity_str) if affinity_str is not None else 0.0

        return {
            "ranking_score": ranking_score,
            "iptm": iptm,
            "affinity": affinity
        }

    except (FileNotFoundError, ValueError, KeyError, IndexError, TypeError) as e:
        print(f"[WARNING] Failed to parse Excel score file: {filepath}. Error: {e}")
        return {"ranking_score": 0.0, "iptm": 0.0, "affinity": 0.0}

def custom_sigmoid(x, threshold, k):
    try:
        return 1.0 / (1.0 + math.exp(-k * (x - threshold)))
    except OverflowError:
        return 0.0 if x < threshold else 1.0

def calculate_composite_reward(all_scores):
    global TRAINING_METRICS_HISTORY
    iptm = all_scores.get("iptm", 0.0)
    ranking_score = all_scores.get("ranking_score", 0.0)
    binding_affinity = all_scores.get("affinity", 0.0)
    W_IPTM, W_RANKING, SIGMOID_THRESHOLD, SIGMOID_STEEPNESS = 0.3, 0.7, 0.73, 50
    structure_quality_score = (W_IPTM * iptm) + (W_RANKING * ranking_score)
    sigmoid_factor = custom_sigmoid(structure_quality_score, SIGMOID_THRESHOLD, SIGMOID_STEEPNESS)
    affinity_factor = -binding_affinity if binding_affinity < 0 else 0.0
    final_reward = sigmoid_factor * affinity_factor
    metrics_tuple = (final_reward, ranking_score, iptm, binding_affinity)
    TRAINING_METRICS_HISTORY.append(metrics_tuple)
    print(f"  [Metrics Logged] Reward: {final_reward:.3f}, RankScore: {ranking_score:.3f}, ipTM: {iptm:.3f}, Affinity: {binding_affinity:.2f} kcal/mol")
    return final_reward

def get_reward_from_shell_script(sequence):
    """
    Call shell script for a single sequence, parse results, and return composite reward.
    """
    shell_script_path = "/data/run01/scz0sfc/af3/get_reward_ES_1.sh"
    base_data_dir = f"/data/run01/scz0sfc/af3/data/{GROUP_NAME}"  # Modified for group-specific directory
    job_id = f"py_{int(time.time())}_{random.randint(1000, 9999)}"  # Add random number to avoid conflicts
    print(f"  Generated unified Job ID for shell script: {job_id}")
    temp_job_dir = os.path.join(base_data_dir, job_id)

    try:
        print(f"  Executing shell script for sequence: {sequence[:10]}...")
        submit_cmd = ["sbatch", "--wait", f"--export=ALL,PARENT_JOB_ID={job_id},GROUP_NAME={GROUP_NAME}", shell_script_path, sequence]
        subprocess.run(submit_cmd, capture_output=True, text=True, check=True)
        print(f"  Shell script job (unified ID: {job_id}) completed.")

        # Find and parse output files
        pipeline_output_dir = os.path.join(temp_job_dir, "af3_pipeline", "output")
        excel_files = glob.glob(os.path.join(pipeline_output_dir, "*.xlsx"))
        
        print(f"Looking for Excel files in: {pipeline_output_dir}")
        if not excel_files:
            raise FileNotFoundError(f"summary.xlsx file not found in {pipeline_output_dir}")

        # Add retry mechanism for corrupted Excel files
        max_retries = 3
        retry_delay = 5  # Retry interval in seconds
        
        for attempt in range(max_retries):
            try:
                # Call parsing function
                all_scores = parse_all_scores_from_excel(excel_files[0])
                break  # Break retry loop if successful
            except (openpyxl.utils.exceptions.InvalidFileException, zipfile.BadZipFile, KeyError, ValueError) as e:
                if attempt < max_retries - 1:
                    print(f"  [Retry {attempt+1}/{max_retries}] Excel file parsing failed: {e}, retrying in {retry_delay} seconds...")
                    time.sleep(retry_delay)
                    # Re-check if file exists
                    excel_files = glob.glob(os.path.join(pipeline_output_dir, "*.xlsx"))
                    if not excel_files:
                        raise FileNotFoundError(f"summary.xlsx file not found in {pipeline_output_dir}")
                else:
                    print(f"  [ERROR] Unable to parse Excel file after {max_retries} retries")
                    return 0.0
        
        # Calculate composite reward
        reward = calculate_composite_reward(all_scores)
        return reward

    except subprocess.CalledProcessError as e:
        print(f"[!!] Shell script execution failed! Stderr: {e.stderr}")
        return 0.0
    except Exception as e:
        print(f"[!!] Unknown error during get_reward: {e}")
        return 0.0
    finally:
        if temp_job_dir:
            print(f"  [Debug Mode] Intermediate files preserved at: {temp_job_dir}")

def get_reward(sequences, device='cuda'):
    """Get rewards for a batch of sequences (executed serially)."""
    rewards = []
    print(f"\nStarting reward calculation for {len(sequences)} sequences (serial execution)...")
    for i, seq in enumerate(sequences):
        print(f"Processing sequence {i+1}/{len(sequences)}")
        reward = get_reward_from_shell_script(seq)
        rewards.append(reward)
    print("All sequence reward calculations completed.")
    return torch.tensor(rewards, dtype=torch.float, device=device)

class EvolutionaryStrategySelector:
    """
    Evolutionary strategy-based site selector.
    """
    def __init__(self, seq_len, exploration_noise=0.5, learning_rate=0.2, decay_rate=0.99):
        self.seq_len = seq_len
        # Use random initial values
        self.values = np.random.uniform(-1, 1, seq_len)
        self.base_exploration_noise = exploration_noise
        self.learning_rate = learning_rate
        self.decay_rate = decay_rate
        self.history = []
        self.step = 0
        
    def select_sites(self, top_k=15):
        """
        Adaptive exploration noise.
        """
        current_noise = self.base_exploration_noise * (self.decay_rate ** self.step)
        
        noisy_values = self.values + np.random.normal(0, current_noise, self.seq_len)
        
        selected_sites = []
        remaining_indices = list(range(self.seq_len))
        
        for _ in range(top_k):
            if not remaining_indices:
                break
                
            max_idx = remaining_indices[np.argmax(noisy_values[remaining_indices])]
            selected_sites.append(max_idx)
            remaining_indices.remove(max_idx)
        
        return sorted(selected_sites)
    
    def update(self, selected_sites, reward):
        """
        Update strategy.
        """
        normalized_reward = np.tanh(reward / 10)
        
        # Update selected sites
        for site in selected_sites:
            self.values[site] += self.learning_rate * (normalized_reward - self.values[site])
        
        # Penalize unselected sites
        all_sites = set(range(self.seq_len))
        unselected_sites = all_sites - set(selected_sites)
        
        for site in unselected_sites:
            self.values[site] -= self.learning_rate * 0.1 * normalized_reward
        
        # Record history
        self.history.append({
            'selected_sites': selected_sites,
            'reward': reward,
            'values': self.values.copy()
        })
        
        self.step += 1
    
    def get_top_sites(self, top_k=15):
        """Get sites with highest current values (without adding noise)."""
        return sorted(np.argsort(self.values)[-top_k:])

'''
================================================================================
SECTION 2: GRPO Reinforcement Learning Main Process
================================================================================
'''

def grpo_finetune_evodiff():
    """
    Main function for fine-tuning EvoDiff model using GRPO algorithm.
    """
    # 1. Hyperparameter Settings
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    learning_rate = 1e-6
    training_steps = 1000
    num_generations = 4
    num_paths_per_variant = 4
    beta = 0.3

    print(f"Using device: {device}")

    # 2. Model and Tokenizer Loading
    print("Loading pretrained model...")
    model, _, tokenizer, _ = OA_DM_38M()
    model.to(device)
    model.train()

    ref_model = copy.deepcopy(model)
    ref_model.to(device)
    ref_model.eval()

    # 3. Evolutionary Strategy Selector Initialization
    print("Initializing evolutionary strategy selector...")
    base_sequence = "PIAQIHILEGRSDEQKETLIREVSEAISRSLDAPLTSVRVIITEMAKGHFGIGGELASK"
    seq_len = len(base_sequence)
    num_mutations = 30
    
    selector = EvolutionaryStrategySelector(
        seq_len=seq_len,
        exploration_noise=1.0,
        learning_rate=0.3,
        decay_rate=0.995
    )

    # 4. Optimizer Setup
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)

    print(f"Original sequence: {base_sequence}")
    print("-" * 50)
    
    losses_history, rewards_history, kls_history = [], [], []
    selected_sites_history = []
    top_sites_history = []
    
    # Record all sequences and related information
    all_sequences = []

    # Create directories for saving plots
    os.makedirs("training_plots", exist_ok=True)
    os.makedirs("all_sequences_ES_1", exist_ok=True)

    # 5. GRPO Training Loop
    for step in range(training_steps):
        # a. Use evolutionary strategy selector to choose mutation sites
        positions_to_mutate = selector.select_sites(top_k=num_mutations)
        selected_sites_history.append(positions_to_mutate)
        
        # Record highest value sites without noise
        top_sites = selector.get_top_sites(top_k=num_mutations)
        top_sites_history.append(top_sites)
        
        # b. Generation (Rollout) - Using single random path
        generation_order = positions_to_mutate.copy()
        random.shuffle(generation_order)
        
        variants = []
        for _ in range(num_generations):
            variant = generate_variant(
                model=model,
                sequence=base_sequence,
                mask_positions=positions_to_mutate,
                decode_order=generation_order,
                tokenizer=tokenizer,
                device=device
            )
            variants.append(variant)

        # c. Reward Calculation (via AlphaFold3 and PRODIGY)
        rewards = get_reward(variants, device=device)

        # Record all sequences and related information
        for i, variant in enumerate(variants):
            reward_val = rewards[i].item()
            # Ensure all values are Python native types
            all_sequences.append({
                'sequence': variant,
                'reward': float(reward_val),
                'step': int(step),
                'positions': [int(pos) for pos in positions_to_mutate]
            })
            
            # Save all sequences every 10 steps
            if step % 10 == 0:
                with open(f"all_sequences_ES_1/all_sequences_step_{step}.json", "w") as f:
                    # Sort by reward
                    sorted_sequences = sorted(all_sequences, key=lambda x: x['reward'], reverse=True)
                    # Ensure all data is serializable
                    json_serializable = []
                    for seq_info in sorted_sequences:
                        json_serializable.append({
                            'sequence': seq_info['sequence'],
                            'reward': float(seq_info['reward']),
                            'step': int(seq_info['step']),
                            'positions': [int(pos) for pos in seq_info['positions']]
                        })
                    json.dump(json_serializable, f, indent=2)

        # d. Advantage Calculation (Within-group normalization)
        mean_reward = rewards.mean()
        std_reward = rewards.std()
        advantages = (rewards - mean_reward) / (std_reward + 1e-8)

        # e. Loss Calculation
        batch_loss = 0
        batch_kl_div = 0

        # Set mini-batch size
        mini_batch_size = 2  # Can be adjusted based on memory

        # Iterate through each generated variant and its corresponding advantage
        for i in range(num_generations):
            variant_sequence = variants[i]
            advantage = advantages[i]

            # For this variant, sample multiple paths for training
            # Use mini-batch processing to reduce memory usage
            num_mini_batches = (num_paths_per_variant + mini_batch_size - 1) // mini_batch_size
            
            for batch_idx in range(num_mini_batches):
                start_idx = batch_idx * mini_batch_size
                end_idx = min((batch_idx + 1) * mini_batch_size, num_paths_per_variant)
                
                mini_batch_loss = 0
                mini_batch_kl_div = 0
                
                # Process current mini-batch
                for _ in range(start_idx, end_idx):
                    # Randomly sample a new training path
                    training_order = positions_to_mutate.copy()
                    random.shuffle(training_order)
                    
                    # Calculate logp using current model
                    with torch.set_grad_enabled(True):
                        log_probs = get_log_probs(model, variant_sequence, training_order, tokenizer, device)
                    
                    # Calculate ref_logp using reference model
                    with torch.no_grad():
                        ref_log_probs = get_log_probs(ref_model, variant_sequence, training_order, tokenizer, device)

                    policy_loss = -(log_probs * advantage).sum()
                    
                    kl_div_per_token = torch.exp(ref_log_probs - log_probs) - (ref_log_probs - log_probs) - 1
                    kl_div = kl_div_per_token.sum()
                    
                    loss = policy_loss + beta * kl_div
                    mini_batch_loss += loss
                    mini_batch_kl_div += kl_div.item()
                
                # Average mini-batch loss and backpropagate
                mini_batch_loss = mini_batch_loss / (end_idx - start_idx)
                mini_batch_loss.backward()
                
                # Accumulate losses
                batch_loss += mini_batch_loss.item() * (end_idx - start_idx)
                batch_kl_div += mini_batch_kl_div
                
                # Clear computation graph to free memory
                del mini_batch_loss
                torch.cuda.empty_cache()

        # Average loss over all variants and all paths
        total_loss = batch_loss / (num_generations * num_paths_per_variant)
        avg_kl_div = batch_kl_div / (num_generations * num_paths_per_variant)

        # f. Update model parameters
        optimizer.step()
        optimizer.zero_grad()
        
        # g. Update evolutionary strategy selector
        mean_reward_value = mean_reward.item()
        selector.update(positions_to_mutate, mean_reward_value)
        
        # h. Record metrics
        losses_history.append(total_loss)
        rewards_history.append(mean_reward_value)
        kls_history.append(avg_kl_div)

        # i. Print logs
        if step % 1 == 0:  # Print every step since training steps are reduced
            print(f"Step: {step+1}/{training_steps} | "
                f"Average Reward: {mean_reward_value:.2f} | "
                f"Policy Loss: {total_loss:.4f} | "
                f"KL Divergence: {avg_kl_div:.4f}")
                
        # j. Output plots and site information every 10 steps
        if step % 10 == 0:
            print(f"Step {step} selected mutation sites: {positions_to_mutate}")
            print(f"Step {step} highest value sites: {top_sites}")
            
            # Output current sequence statistics
            if all_sequences:
                sorted_sequences = sorted(all_sequences, key=lambda x: x['reward'], reverse=True)
                top_5_sequences = sorted_sequences[:5]
                print(f"Step {step} top 5 high-reward sequences:")
                for i, seq_info in enumerate(top_5_sequences):
                    print(f"  Rank {i+1}: Reward={seq_info['reward']:.2f}, Step={seq_info['step']}")
            
            # Plot and save current training metrics
            fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))
            fig.suptitle(f'GRPO Evolutionary Strategy (Step {step})', fontsize=16)

            ax1.plot(losses_history, 'b-', label='policy loss')
            ax1.set_xlabel('training steps'); ax1.set_ylabel('loss'); ax1.set_title('Policy Loss'); ax1.legend()
            ax1.grid(True, linestyle='--', alpha=0.7)
            
            ax2.plot(rewards_history, 'g-', label='average reward')
            ax2.set_xlabel('training steps'); ax2.set_ylabel('reward'); ax2.set_title('Average Reward'); ax2.legend()
            ax2.grid(True, linestyle='--', alpha=0.7)

            ax3.plot(kls_history, 'r-', label='KL')
            ax3.set_xlabel('training steps'); ax3.set_ylabel('KL'); ax3.set_title('KL Divergence'); ax3.legend()
            ax3.grid(True, linestyle='--', alpha=0.7)
            
            # Plot site selection changes
            site_changes = np.zeros((len(selected_sites_history), seq_len))
            for i, sites in enumerate(selected_sites_history):
                for site in sites:
                    site_changes[i, site] = 1
            
            ax4.imshow(site_changes.T, aspect='auto', cmap='Blues')
            ax4.set_xlabel('Training Step')
            ax4.set_ylabel('Amino Acid Position')
            ax4.set_title('Site Selection Over Time')

            # Add moving averages
            window = 5
            if len(losses_history) >= window:
                loss_ma = np.convolve(losses_history, np.ones(window)/window, mode='valid')
                ax1.plot(range(window-1, len(losses_history)), loss_ma, 'b--', label='moving average')
                ax1.legend()
                
                reward_ma = np.convolve(rewards_history, np.ones(window)/window, mode='valid')
                ax2.plot(range(window-1, len(rewards_history)), reward_ma, 'g--', label='moving average')
                ax2.legend()
                
                kl_ma = np.convolve(kls_history, np.ones(window)/window, mode='valid')
                ax3.plot(range(window-1, len(kls_history)), kl_ma, 'r--', label='moving average')
                ax3.legend()

            plt.tight_layout()
            plt.savefig(f"training_plots/grpo_evolutionary_strategy_training_metrics_step_{step}.png", dpi=300, bbox_inches='tight')
            plt.close(fig)  # Close plot to avoid memory leaks

    # 6. Generate Final Plots After Training
    print("Training completed, generating final plots...")
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))
    fig.suptitle('GRPO with Evolutionary Strategy Selector (Final)', fontsize=16)

    ax1.plot(losses_history, 'b-', label='policy loss')
    ax1.set_xlabel('training steps'); ax1.set_ylabel('loss'); ax1.set_title('Policy Loss'); ax1.legend()
    ax1.grid(True, linestyle='--', alpha=0.7)
    
    ax2.plot(rewards_history, 'g-', label='average reward')
    ax2.set_xlabel('training steps'); ax2.set_ylabel('reward'); ax2.set_title('Average Reward'); ax2.legend()
    ax2.grid(True, linestyle='--', alpha=0.7)

    ax3.plot(kls_history, 'r-', label='KL')
    ax3.set_xlabel('training steps'); ax3.set_ylabel('KL'); ax3.set_title('KL Divergence'); ax3.legend()
    ax3.grid(True, linestyle='--', alpha=0.7)
    
    # Plot site selection changes
    site_changes = np.zeros((len(selected_sites_history), seq_len))
    for i, sites in enumerate(selected_sites_history):
        for site in sites:
            site_changes[i, site] = 1
    
    ax4.imshow(site_changes.T, aspect='auto', cmap='Blues')
    ax4.set_xlabel('Training Step')
    ax4.set_ylabel('Amino Acid Position')
    ax4.set_title('Site Selection Over Time')
    
    plt.tight_layout()
    plt.savefig("grpo_evolutionary_strategy_training_metrics_final.png", dpi=300, bbox_inches='tight')
    plt.show()
    
    # Plot site selection heatmap and value changes
    plot_site_selection_analysis(selector, selected_sites_history, top_sites_history, seq_len)
    
    # Output sites selected in last 10 steps
    print("\nSites selected in last 10 steps:")
    for i in range(max(0, len(selected_sites_history)-10), len(selected_sites_history)):
        print(f"Step {i}: {selected_sites_history[i]}")
    
    # Output all sequences
    print_all_sequences(all_sequences)

def print_all_sequences(all_sequences):
    """
    Output all sequences and their information.
    """
    if not all_sequences:
        print("No sequences found")
        return
        
    # Sort by reward
    sorted_sequences = sorted(all_sequences, key=lambda x: x['reward'], reverse=True)
    
    print("\n" + "="*80)
    print("All Sequence Rankings")
    print("="*80)
    
    # Output top 50 sequences
    for i, seq_info in enumerate(sorted_sequences[:50]):
        print(f"\nRank {i+1}:")
        print(f"  Reward: {seq_info['reward']:.2f}")
        print(f"  Generation Step: {seq_info['step']}")
        print(f"  Mutation Sites: {seq_info['positions']}")
        print(f"  Sequence: {seq_info['sequence']}")
    
    # Save to file
    with open("all_sequences_ES_1/final_all_sequences.json", "w") as f:
        json.dump(sorted_sequences, f, indent=2)
    
    # Plot reward distribution
    rewards = [s['reward'] for s in sorted_sequences]
    plt.figure(figsize=(10, 6))
    plt.hist(rewards, bins=20, alpha=0.7, color='skyblue', edgecolor='black')
    plt.xlabel('Reward')
    plt.ylabel('Frequency')
    plt.title('Distribution of All Sequence Rewards')
    plt.grid(True, linestyle='--', alpha=0.7)
    plt.savefig("all_sequences_ES_1/reward_distribution.png", dpi=300, bbox_inches='tight')
    plt.show()

def plot_site_selection_analysis(selector, selected_sites_history, top_sites_history, seq_len):
    """
    Plot site selection analysis.
    """
    # Create selection frequency matrix
    selection_freq = np.zeros(seq_len)
    for sites in selected_sites_history:
        for site in sites:
            selection_freq[site] += 1
    
    # Normalize
    selection_freq = selection_freq / len(selected_sites_history)
    
    # Plot selection frequency heatmap
    plt.figure(figsize=(15, 5))
    plt.bar(range(seq_len), selection_freq)
    plt.xlabel('Amino Acid Position')
    plt.ylabel('Selection Frequency')
    plt.title('Site Selection Frequency During Training')
    plt.grid(True, linestyle='--', alpha=0.7)
    plt.savefig("site_selection_frequency_improved_es.png", dpi=300, bbox_inches='tight')
    plt.show()
    
    # Plot final value vector
    plt.figure(figsize=(15, 5))
    plt.bar(range(seq_len), selector.values)
    plt.xlabel('Amino Acid Position')
    plt.ylabel('Final Value')
    plt.title('Final Site Values from Improved Evolutionary Strategy')
    plt.grid(True, linestyle='--', alpha=0.7)
    plt.savefig("final_site_values_improved_es.png", dpi=300, bbox_inches='tight')
    plt.show()
    
    # Plot value change heatmap
    if len(selector.history) > 0:
        # Extract historical values
        value_history = np.array([h['values'] for h in selector.history])
        
        # Plot heatmap
        plt.figure(figsize=(15, 8))
        plt.imshow(value_history.T, aspect='auto', cmap='viridis')
        plt.colorbar(label='Site Value')
        plt.xlabel('Training Step')
        plt.ylabel('Amino Acid Position')
        plt.title('Site Value Evolution During Training')
        plt.savefig("site_value_evolution_improved_es.png", dpi=300, bbox_inches='tight')
        plt.show()
        
    # Plot divergence between selected sites and highest value sites
    divergence = []
    for i in range(len(selected_sites_history)):
        selected_set = set(selected_sites_history[i])
        top_set = set(top_sites_history[i])
        # Calculate Jaccard distance
        divergence.append(1 - len(selected_set & top_set) / len(selected_set | top_set))
    
    plt.figure(figsize=(10, 5))
    plt.plot(divergence, 'r-')
    plt.xlabel('Training Step')
    plt.ylabel('Selection Divergence')
    plt.title('Divergence Between Selected Sites and Top Value Sites')
    plt.grid(True, linestyle='--', alpha=0.7)
    plt.savefig("selection_divergence.png", dpi=300, bbox_inches='tight')
    plt.show()


if __name__ == '__main__':
    grpo_finetune_evodiff()
